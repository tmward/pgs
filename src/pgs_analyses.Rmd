---
title: PGS Analyses
author: Thomas Ward
date: "`r Sys.Date()`"
output: pdf_document
---


```{r, message = FALSE, warning = FALSE}
library(magrittr)
library(dplyr)
library(readr)
library(forcats)
library(tidyr)
library(stringr)
library(ggplot2)
library(rethinking)
library(ggdist)
library(showtext)
font_add_google("Lato")
set_ulam_cmdstan(TRUE)
theme_set(theme_light(base_family = "Lato"))
showtext_auto()
histo_color = "#4C0099"
```

# Prepare modeling data

## Load

```{r}
dat <- readr::read_csv("../data/chole_pgs.csv", col_types = "iiidddddli")
skimr::skim_without_charts(dat)
```

## Keep surgeons with 5 or more cases

Arrange rows by surgeons number of cases
(surgeons with most cases will be at the top, fewest cases at the bottom).
Then replace the `videoid` with a new sequential integer id based on this order.
This will allow us to drop surgeons with few cases and
then still have a nice sequential id to join back on to during analysis of results:

```{r}
dat <- dat %>%
    mutate(surgid = as.integer(fct_infreq(as.character(surgid)))) %>%
    arrange(surgid) %>%
    mutate(videoid = 1:nrow(.))
```

Keep surgeons with 5 or more cases:

```{r}
dat <- dat %>%
    group_by(surgid) %>%
    filter(n() >= 5) %>%
    ungroup()
```

```{r}
nsurgs <- max(dat$surgid)
nsurgs
```

## Prepare data for ulam/stan

### Duration analyses

We performed analyses on the log scale
given we were concerned about the magnitude of duration
and that factors that increase duration tend to do so exponentially.
For example, an inflamed gallbladder is harder to grasp,
but also harder to dissect, and combining the two together,
you have an exponential increase in time.

We also standardize our variables,
as it assists with making weakly regularizing priors:

```{r}
log_duration <- log(dat$laparascopic_duration)
mean_log_duration <- mean(log_duration)
centered_log_duration <- log_duration - mean_log_duration
sd_log_duration <- sd(centered_log_duration)
std_log_duration <- centered_log_duration / sd_log_duration
```

Put list of data for modeling together.
Note the `alpha`, which is our dirichlet prior
(prior of 2 for PGS2, PGS3, PGS4, and PGS5):

```{r}
ddat <- list(
    duration = std_log_duration,
    sid = dat$surgid,
    pgs = dat$pgs,
    alpha = rep(2, 4)
)
```

## GB hole analyses

```{r}
gdat <- list(
    hole = as.integer(dat$gb_hole),
    sid = dat$surgid,
    pgs = dat$pgs,
    alpha = rep(2, 4)
)
```

## CVS attainment analyses

Analyses data is nearly ready,
except for `dat$time_cvs_attained`.

We want to look at the binomial outcome,
of whether or not CVS was achieved.
To do so, just need to transform the data,
so if it's `NA` then the CVS was never achieved.

```{r}
cvs <- as.integer(!is.na(dat$time_cvs_attained))
```

Put the data together for stan:

```{r}
cdat <- list(
    cvs = cvs,
    sid = dat$surgid,
    pgs = dat$pgs,
    alpha = rep(2, 4)
)
```

# Utility functions

## standard error

```{r}
std_err  <- function(xs) {
    sd(xs) / sqrt(length(xs))
}
```


## De-standardize (back to original scale)

Used to move from the standardized/centered log scale to normal scale
to facilitate analyses:

```{r}
unstd <- function(x) {
    exp(x * sd_log_duration + mean_log_duration)
}
```

## Find rows with high pareto k

```{r}
high_k_rows <- function(results) {
    results %>%
        as_tibble(rownames = "videoid") %>%
        mutate(videoid = as.integer(videoid)) %>%
        inner_join(dat, by = "videoid") %>%
        filter(k >= 0.5)
}
```

## samples from stan model

This is a thin wrapper around `rethinking` packages `extract.samples()`
that returns a tibble with `janitor` fixing column names and
a sample number given to each sample.

```{r}
extract_samples <- function(..., seed = 1234) {
    set.seed(seed)
    extract.samples(...) %>%
        as_tibble() %>%
        # case = "none" to not mess up capitalization of our parameters
        # otherwise fixes brackets, commas, and other problem chars
        janitor::clean_names(case = "none") %>%
        mutate(sample_num = paste0("sample", 1:nrow(.)), .before = 1L)
}
```

## tidy surgeons

Each sample has the info for one or more surgeons on each row.
We want this info "tidy", that is, one per row,
so make a function to do that:

```{r}
tidy_surgeons <- function(df) {
    df %>%
        pivot_longer(
            starts_with(c("a_", "bP")),
            names_to = c(".value", "surgeon"),
            names_sep = "_"
        )
}
```

## tidy pgs

Each sample, when using pgs levels, has 5 tidy rows of info,
1 for each PGS.
So need to pivot these longer:

```{r}
tidy_pgs <- function(df) {
    df %>%
        pivot_longer(
            starts_with("PGS"),
            names_to = c(NA, "PGS"),
            names_pattern = "(PGS)([1-5])",
            names_transform = list("PGS" = as.integer),
            # to keep consistent with var name in model definition
            values_to = "sum_delta_j"
        )
}
```

## sum deltas

We treat PGS as an ordered categorical predictor following
McElreath's strategy in "Statistical Rethinking", Chapter 12, section 4.
This assigns a proportion of the maximum value (PGS5) to each of the other PGS's.
PGS1 is absorbed into the intercept.

The below function just calculates this proportion
to make later calculations easier.

```{r}
sum_deltas <- function(df) {
    df %>% mutate(
        PGS1 = 0,
        PGS2 = delta_1,
        PGS3 = PGS2 + delta_2,
        PGS4 = 1 - delta_4,
        PGS5 = 1
    )
}
```

## Plot half eye

This is a wrapper over `stat_halfeye()` that will provide a fill
for both the distribution and the scale with labels for percents.
Color palette is purple gradients.

```{r}
halfeye <- function(df, xcol, ycol) {
    ggplot(df, aes(y = {{ ycol }}, x = {{ xcol }})) +
    stat_halfeye(
        aes(
            fill = stat(
                cut_cdf_qi(cdf, .width = c(0.5, 0.8, 0.95), labels = scales::percent_format())
            )
        ),
        .width = c(0.5, 0.8, 0.95)
    ) +
    scale_fill_brewer(direction = -1, palette = "Purples", na.translate = FALSE) +
    labs(fill = "Compatibility\nIntervals")
}
```

## Compatibility intervals

Quickly calculate a bunch of compatibility intervals and pretty format them:

```{r}
ci_ints  <- function(df, variable) {
    summarise(df,
        mean = mean({{ variable }}),
        low_50 = quantile({{ variable }}, probs = 0.25),
        high_50 = quantile({{ variable }}, probs = 0.75),
        low_66 = quantile({{ variable }}, probs = 0.17),
        high_66 = quantile({{ variable }}, probs = 0.83),
        low_80 = quantile({{ variable }}, probs = 0.20),
        high_80 = quantile({{ variable }}, probs = 0.80),
        low_89 = quantile({{ variable }}, probs = 0.055),
        high_89 = quantile({{ variable }}, probs = 0.945),
        low_95 = quantile({{ variable }}, probs = 0.025),
        high_95 = quantile({{ variable }}, probs = 0.975)
    ) %>%
    ungroup() %>%
    mutate(across(where(is.numeric), ~ round(., 2))) %>%
    unite(int_50, contains("_50"), sep = ", ") %>%
    unite(int_66, contains("_66"), sep = ", ") %>%
    unite(int_80, contains("_80"), sep = ", ") %>%
    unite(int_89, contains("_89"), sep = ", ") %>%
    unite(int_95, contains("_95"), sep = ", ")
}
```


# Abbreviations in formulas

- `sid`: Deidentified surgeon id
- `PGS`: Parkland Grading Scale for gallbladder inflammation
- `MVNormal`: Multivariate normal distribution
- `LKJCorr`: Lewandowski, Kurowicka, and Joe Correlation Distribution
- `CVS`: Critical View of Safety

# Duration Analysis

## Priors determination
We will use weakly regularizing priors, that is,
those that constrain parameters to those that are logically possible,
while still allowing for some implausibly strong relationships
if that is what the data determines.

I will graph simulated values of the priors to help determine good ones to use:

Try a prior of N(0, 1) for the intercept (`a`) and
N(0, 0.3) for the slope for PGS5, `bp`:

```{r}
set.seed(1234)
max_dur <- max(ddat$duration)
min_dur <- min(ddat$duration)
tibble(
    sample_num = paste0("sample", 1:50),
    a = rnorm(50, mean = 0, sd = 1),
    bp = rnorm(50, mean = 0, sd = 0.3)
) %>%
    mutate(x = list(seq(from = -0.1, to = 1.1, length.out = 30))) %>%
    tidyr::unnest(x) %>%
    mutate(y = a + bp * x, og_y = unstd(y)) %>%
    ggplot(aes(x, og_y, group = sample_num)) +
    geom_line() +
    scale_y_log10() +
    labs(y = "y (minutes)") +
    geom_hline(yintercept = unstd(c(min_dur, max_dur)), linetype = 2, color = "blue")
```

The intercept prior looks acceptable.
The slope prior is much too tight
(remember slope is for a PGS5, so the maximum effect).
The min/max of data never even reached.
Let's try making it a bit bigger:


```{r}
set.seed(1234)
max_dur <- max(ddat$duration)
min_dur <- min(ddat$duration)
tibble(
    sample_num = paste0("sample", 1:50),
    a = rnorm(50, mean = 0, sd = 1),
    bp = rnorm(50, mean = 0, sd = 1.2)
) %>%
    mutate(x = list(seq(from = -0.1, to = 1.1, length.out = 30))) %>%
    tidyr::unnest(x) %>%
    mutate(y = a + bp * x, og_y = unstd(y)) %>%
    ggplot(aes(x, og_y, group = sample_num)) +
    geom_line() +
    #scale_y_log10() +
    #labs(y = "y (log scale, minutes)") +
    labs(y = "y (minutes)") +
    geom_hline(yintercept = unstd(c(min_dur, max_dur)), linetype = 2, color = "blue")
```

The numbers are now constrained to mostly realistic values.

Other priors will be the usual weakly regularizing ones,
including Dirichlet of 2,
Exponential 1,
and LKJCorr of 4.

## Formula

Below is the centered version.
The model given to Stan is the non-centered version
that is mathematically equivalent but dramatically improves sampling.

\begin{equation}
\begin{split}
log(Duration_i) &\sim Normal(\mu_i, \sigma) \\
\mu_i &= \alpha_{sid[i]} + \beta_{sid[i]} * \sum_{j = 0}^{PGS_i - 1} \delta_j \\
\begin{bmatrix}
    \alpha_{sid} \\
    \beta_{sid} \\
\end{bmatrix} &\sim MVNormal(
    \begin{bmatrix}
        \alpha \\
        \beta
    \end{bmatrix},
    \mathbf{S}
    ) \\
\alpha &\sim Normal(0, 1) \\
\beta &\sim Normal(0, 1.2) \\
\delta &\sim Dirichlet(2) \\
\mathbf{S} &=
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} 
  \mathbf{R}
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} \\
\mathbf{R} &=
  \begin{pmatrix}
    1 & \rho \\
    \rho & 1
  \end{pmatrix} \\
\mathbf{R} &\sim LKJCorr(4) \\
\sigma, \sigma_{\alpha}, \sigma_{\beta} &\sim Exponential(1)
\end{split}
\end{equation}


## Code

```{r}
set.seed(1234)
dur_mod <- ulam(
    alist(
        duration ~ normal(mu, sigma),
        mu <- a_bar + ab_sid[sid, 1] + (bP_bar + ab_sid[sid, 2]) * sum(delta_j[1:pgs]),
        transpars> matrix[sid, 2]:ab_sid <-
             compose_noncentered(sigma_sid, L_Rho_sid, z_sid),
        matrix[2, sid]:z_sid ~ normal(0, 1),
        a_bar ~ normal(0, 1),
        bP_bar ~ normal(0, 1.2),
        vector[2]:sigma_sid ~ exponential(1),
        sigma ~ exponential(1),
        cholesky_factor_corr[2]:L_Rho_sid ~ lkj_corr_cholesky(4),
        vector[5]:delta_j <<- append_row(0, delta),
        simplex[4]:delta ~ dirichlet(alpha),
        # compute correlation matrix from Cholesky matrix
        gq> matrix[2, 2]: Rho_sid <<- Chol_to_Corr(L_Rho_sid),
        # for our analysis sake, compute a[sid] and b[sid]
        gq> vector[sid]:a <<- a_bar + ab_sid[, 1],
        gq> vector[sid]:bP <<- bP_bar + ab_sid[, 2]
    ),
    data = ddat,
    cores = 4,
    chains = 4,
    iter = 5000,
    log_lik = TRUE
)
```

## Diagnostic Evaluation of Markov Chains

### Rhat4 and effective sampling size

```{r}
precis(dur_mod, depth = 3)
```

All Rhat4 values are 1.

Each parameter also sampled well.

### PSIS/WAIC

```{r}
PSIS(dur_mod)
WAIC(dur_mod)
```

The are some Pareto k values > 0.5. As long as < 0.7, not an issue.
Are they?

```{r}
PSIS(dur_mod, pointwise = TRUE) %>%
    high_k_rows()
```

Only 2 rows total, and both have k < 0.6,
so minimal issue with outliers.

### Trace rank plot (trankplot)

```{r, eval = FALSE}
trankplot(dur_mod)
```

Good mixing as shown with large amount of overlap.

### Trace plot

```{r, eval = FALSE}
traceplot(dur_mod)
```

Chains are stationary with a visible central tendency,
have good mixing, and converge. Excellent.

## Evaluate model results

List of the parameters I care about from the model:

1. Intercepts (`a[N]` and average `a_bar`)
2. beta for pgs5 (`bP[N]` and average `bP_bar`)
3. Correlation (`Rho_sid[1, 2]`)
4. sigmas (overall, for intercept, for beta)
5. deltas (for incremental effect of each PGS)

```{r}
pars <- c(
    "a_bar",
    paste0("a[", 1:nsurgs, "]"),
    "bP_bar",
    paste0("bP[", 1:nsurgs, "]"),
    # sigma for entire model
    "sigma",
    # sigma_a
    "sigma_sid[1]",
    # sigma_b
    "sigma_sid[2]",
    paste0("Rho_sid[1,2]"),
    paste0("delta[", 1:4, "]")
)
```

Now time to extract samples from stan:

```{r}
dur_samples <- extract_samples(dur_mod, pars = pars) %>%
    # change names for ease of remembering rather than having to
    # remember their position in the matrices
    rename(sigma_a = sigma_sid_1, sigma_bP = sigma_sid_2, rho = Rho_sid_1_2) %>%
    sum_deltas()
```

Now, let us generate a "new" unseen surgeon
to see what the model will predict for a new surgeon,
make the data tidy (one row per surgeon per PGS per sample),
then calculate the outcome from the samples to see the model's predictions.

```{r}
tidy_dur_samples <- dur_samples %>%
    # generates a "new" unseen surgeon 
    mutate(
        a_new = rnorm(nrow(.), a_bar, sigma_a),
        bP_new = rnorm(nrow(.), bP_bar, sigma_bP)
    ) %>%
    select(sample_num, starts_with(c("a_", "bP", "PGS")), sigma) %>%
    tidy_surgeons() %>%
    tidy_pgs() %>%
    mutate(mu = a + bP * sum_delta_j, mu_og = unstd(mu)) %>%
    select(sample_num, surgeon, PGS, mu, mu_og, sigma)
```

## Plot results

### Was duration correlated with starting time?

```{r}
extract_samples(dur_mod, pars = c("Rho_sid[1,2]")) %>%
    rename(rho = Rho_sid_1_2) %>%
    ci_ints(rho) %>%
    knitr::kable(
        caption = "Correlation between time to perform PGS1 case and the effect of incrementing PGS on laparoscopic duration"
    )
```

### For every surgeon
Visualization of the effects PGS has on each surgeon.
Surgeon `bar` is the average surgeon, and
surgeon `new` is for a future never seen surgeon.

```{r}
tidy_dur_samples %>%
    mutate(surgeon = parse_factor(surgeon, levels = c("bar", as.character(1:nsurgs), "new"))) %>%
    ggplot(aes(PGS, mu_og)) +
    stat_lineribbon() +
    facet_wrap(~ surgeon, scales = "free_y") +
    scale_fill_brewer(labels = c("95%", "80%", "50%"), palette = "Purples") +
    labs(y = "Case duration (min)")
```

### Calculate deltas
Calculate the change from a PGS1 each PGS level has.

First, who is affected the most and the least by PGS?
This will manifest by the smallest and the largest `bP`:

```{r, warning = FALSE}
dur_bps <- precis(dur_mod, depth = 2, pars = paste0("bP[", 1:nsurgs, "]")) %>%
    as_tibble(rownames = "var")
# Least affected surgeon:
slice_min(dur_bps, mean)
# Most affected surgeon:
slice_max(dur_bps, mean)
```

So the most affected surgeon is Surgeon "4"
and the least affected surgeon is Surgeon "5".

Now actually calculate the deltas:

```{r}
dur_pgs_deltas <- tidy_dur_samples %>%
    # selecting our least, average, and most affected surgeons
    filter(surgeon %in% c("bar", "4", "5")) %>%
    mutate(
        surgeon = case_when(
            surgeon == "4" ~ "Most affected",
            surgeon == "5" ~ "Least affected",
            surgeon == "bar" ~ "Average"
        ),
        surgeon = parse_factor(
            surgeon,
            levels = c("Least affected", "Average", "Most affected")
        )
    ) %>%
    # ensure correct order, with PGS1 as first row in future group
    # will rely on this ordering to calculate delta
    arrange(sample_num, surgeon, PGS) %>%
    group_by(sample_num, surgeon) %>%
    mutate(pgs_delta = mu_og - mu_og[1]) %>%
    ungroup() %>%
    # needed for ggdist to work
    mutate(PGS = as.factor(PGS))
```

### On average across all surgeons

How was the average surgeon affected?

Then plot this:

```{r}
dur_pgs_deltas_bar  <- filter(dur_pgs_deltas, surgeon == "Average", PGS != "1")
```

```{r}
dur_pgs_deltas_bar %>%
    halfeye(pgs_delta, PGS) +
    coord_cartesian(xlim = c(0, 40)) +
    labs(
        x = "Added time to case (minutes)",
        y = "Parkland Grading Scale"
    )
ggsave("../output/pgs_duration.svg", width = 6, height = 6)
ggsave("../output/pgs_duration.pdf", width = 6, height = 6)
```

What are the numbers of the compatibility intervals:

```{r}
dur_pgs_deltas_bar %>%
    group_by(PGS) %>%
    ci_ints(pgs_delta) %>%
    knitr::kable()
```

All units are in minutes.

### surgeon most affected

How much was the most affected surgeon, affected?

Then plot this:

```{r}
dur_pgs_deltas_ma  <- filter(dur_pgs_deltas, surgeon == "Most affected", PGS != "1")
```

```{r}
dur_pgs_deltas_ma %>%
    halfeye(pgs_delta, PGS) +
    coord_cartesian(xlim = c(0, 70)) +
    labs(
        x = "Added time to case (min)"
    )
```

What are the numbers of the compatibility intervals:

```{r}
dur_pgs_deltas_ma %>%
    group_by(PGS) %>%
    ci_ints(pgs_delta) %>%
    knitr::kable()
```

All units are in minutes.

### surgeon least affected

How much was the least affected surgeon, affected?

Then plot this:

```{r}
dur_pgs_deltas_la  <- filter(dur_pgs_deltas, surgeon == "Least affected", PGS != "1")
```

```{r}
dur_pgs_deltas_la %>%
    halfeye(pgs_delta, PGS) +
    coord_cartesian(xlim = c(-20, 30)) +
    labs(
        x = "Added time to case (min)"
    )
```

What are the numbers of the compatibility intervals:

```{r}
dur_pgs_deltas_la %>%
    group_by(PGS) %>%
    ci_ints(pgs_delta) %>%
    knitr::kable()
```

All units are in minutes.

### all together now

```{r}
dur_pgs_deltas %>%
    filter(PGS != "1") %>%
    halfeye(pgs_delta, PGS) +
    facet_wrap(~ surgeon, ncol = 1) +
    coord_cartesian(xlim = c(-20, 70)) +
    labs(
        x = "Added time to case (min)"
    )
```

```{r}
dur_pgs_deltas %>%
    mutate(PGS = as.integer(PGS)) %>%
    ggplot(aes(PGS, mu_og)) +
    stat_lineribbon() +
    facet_wrap(~ surgeon, ncol = 1) +
    scale_fill_brewer(labels = c("95%", "80%", "50%"), palette = "Purples") +
    labs(y = "Case duration (min)", fill = "Compatibility\nIntervals")
```

# Gallbladder Holes Analysis

We treated a gallbladder hole
occurring during the removal of the gallbladder bed as a binomial outcome,
that is, we cared if 1 or more holes occurred and treated those equal.
We chose this because once a full-thickness perforation occurs,
bile/stones spill.
Adding further holes at this point is rather inconsequential,
as what matters is going from no hole to a single hole.

## Priors determination
As before, we will use weakly regularizing priors.

### Intercept

To see how `logit()` can mess up priors,
look at a N(0, 10) intercept prior:

```{r}
p <- rnorm(1000, 0, 10)
hist(inv_logit(p))
```

Nearly all the probability mass is at 0 or 1 for the prior,
which is nonsensical.

Trying a N(0, 1) prior:


```{r}
p <- rnorm(1000, 0, 1)
hist(inv_logit(p))
```

The bulk of the mass is too in the middle,
that is,
it is estimating the probability of a GB hole to be around 50%.
a priori, that seems unlikely.

A N(0, 1.5) is much more even and the one we will use:

```{r}
p <- rnorm(1000, 0, 1.5)
hist(inv_logit(p))
```

### PGS coefficient

Remember, this is the effect of a PGS5.
We will need to simulate the intercept then add the slope,
followed by undoing the logit to see the probability:

A N(0, 1)

```{r}
pint <- rnorm(1000, 0, 1.5)
ppgs <- rnorm(1000, 0, 1)
pgsdiff <- inv_logit(pint + ppgs) - inv_logit(pint)
hist(pgsdiff)
```

Is fairly reasonable (PGS5 does on average no change in getting a GB
hole, but can change it by 60% probability in rare instances).

That seems a little too strong of an effect, so let's decrease with N(0, 0.75):

```{r}
pint <- rnorm(1000, 0, 1.5)
ppgs <- rnorm(1000, 0, 0.75)
pgsdiff <- inv_logit(pint + ppgs) - inv_logit(pint)
hist(pgsdiff)
```

### other priors
Other priors will be the usual weakly regularizing ones,
including Dirichlet of 2,
Exponential 1,
and LKJCorr of 4.

## Formula

Below is the centered version.
The model given to Stan is the non-centered version
that is mathematically equivalent but dramatically improves sampling.

\begin{equation}
\begin{split}
Hole_i &\sim Bernoulli(p_i) \\
logit(p_i) &= \alpha_{sid[i]} + \beta_{sid[i]} * \sum_{j = 0}^{PGS_i - 1} \delta_j \\
\begin{bmatrix}
    \alpha_{sid} \\
    \beta_{sid} \\
\end{bmatrix} &\sim MVNormal(
    \begin{bmatrix}
        \alpha \\
        \beta
    \end{bmatrix},
    \mathbf{S}
    ) \\
\alpha &\sim Normal(0, 1.5) \\
\beta &\sim Normal(0, 0.75) \\
\delta &\sim Dirichlet(2) \\
\mathbf{S} &=
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} 
  \mathbf{R}
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} \\
\mathbf{R} &=
  \begin{pmatrix}
    1 & \rho \\
    \rho & 1
  \end{pmatrix} \\
\mathbf{R} &\sim LKJCorr(4) \\
\sigma_{\alpha}, \sigma_{\beta} &\sim Exponential(1)
\end{split}
\end{equation}

## Code

```{r}
set.seed(1234)
hole_mod <- ulam(
    alist(
        hole ~ bernoulli(p),
        logit(p) <- a_bar + ab_sid[sid, 1] + (bP_bar + ab_sid[sid, 2]) * sum(delta_j[1:pgs]),
        a_bar ~ normal(0, 1.5),
        bP_bar ~ normal(0, 0.75),
        vector[5]: delta_j <<- append_row(0, delta),
        simplex[4]: delta ~ dirichlet(alpha),
        transpars> matrix[sid, 2]: ab_sid <-
            compose_noncentered(sigma_sid, L_Rho_sid, z_sid),
        matrix[2, sid]: z_sid ~ normal(0, 1),
        vector[2]: sigma_sid ~ exponential(1),
        cholesky_factor_corr[2]: L_Rho_sid ~ lkj_corr_cholesky(4),
        gq> matrix[2, 2]: Rho_sid <<- Chol_to_Corr(L_Rho_sid),
        gq> vector[sid]:a <<- a_bar + ab_sid[, 1],
        gq> vector[sid]:bP <<- bP_bar + ab_sid[, 2]
    ),
    data = gdat,
    cores = 4,
    chains = 4,
    iter = 5000,
    log_lik = TRUE
)
```

## Diagnostic Evaluation of Markov Chains

### Rhat4 and effective sampling size

```{r}
precis(hole_mod, depth = 3)
```

All Rhat4 values are 1.

Each parameter also sampled well.

### PSIS/WAIC

```{r}
PSIS(hole_mod)
WAIC(hole_mod)
```

The are some Pareto k values > 0.5. As long as < 0.7, not an issue.
Are they?

```{r}
PSIS(hole_mod, pointwise = TRUE) %>%
    high_k_rows()
```

Only 2 rows total, and both have k < 0.6,
so minimal issue with outliers.

### Trace rank plot (trankplot)

```{r, eval = FALSE}
trankplot(hole_mod)
```

Good mixing as shown with large amount of overlap.

### Trace plot

```{r, eval = FALSE}
traceplot(hole_mod)
```

Chains are stationary with a visible central tendency,
have good mixing, and converge.

## Evaluate model results
First, obtain tidy samples, as we did with the duration model.
Additionally, transform the result to get the absolute probability of a hole
and the OR of a hole for each PGS:

```{r}
tidy_hole_samples <- extract_samples(
    hole_mod,
    pars = c(
        "a_bar",
        paste0("a[", 1:nsurgs, "]"),
        "bP_bar",
        paste0("bP[", 1:nsurgs, "]"),
        # sigma_a
        "sigma_sid[1]",
        # sigma_b
        "sigma_sid[2]",
        paste0("Rho_sid[1,2]"),
        paste0("delta[", 1:4, "]")
    )
) %>%
    tidy_surgeons() %>%
    sum_deltas() %>%
    tidy_pgs() %>%
    mutate(
        p = inv_logit(a + bP * sum_delta_j),
        bOR = exp(bP * sum_delta_j)
    ) %>%
    arrange(sample_num, surgeon, PGS) %>%
    group_by(surgeon, sample_num) %>%
    # calc change in probability for each sample
    # group by surgeon as well because each sample_num
    # has the info for all 10 surgeons and bar surgeon
    mutate(
        # change from one PGS level to the other
        incr_delta_p = p - lag(p),
        # change from PGS1 to that PGS level
        delta_p = p - p[1]
    ) %>%
    ungroup()
```

Key to know, is that:

`incr_delta_p` is the probability difference from one level to the next.
It will be `NA` for a PGS1.

`delta_p` is probability difference from a certain PGS level to PGS1.

`p` is the probability of a hole at that level

`bOR` is the increased odds, compared to a PGS of 1, of a gb hole.

## Plot results

### For all surgeons

Remember that "bar" is the average of all the surgeons:

```{r}
tidy_hole_samples %>%
    mutate(surgeon = parse_factor(surgeon, levels = c("bar", as.character(1:nsurgs)))) %>%
    ggplot(aes(PGS, p)) +
    stat_lineribbon() +
    facet_wrap(~ surgeon) +
    scale_fill_brewer(palette = "Purples") +
    labs(y = "Probability of GB hole")
```

### On average across all surgeons

```{r}
bar_gb <- tidy_hole_samples %>%
    filter(surgeon == "bar", PGS != 1) %>%
    mutate(PGS = as.factor(PGS))
bar_gb %>%
    halfeye(delta_p, PGS) +
    coord_cartesian(xlim = c(-0.2, 0.4)) +
    labs(
        y = "PGS",
        x = "Added probability of a gallbladder hole"
    )
```

And numbers for this:

```{r}
bar_gb %>%
    group_by(PGS) %>%
    ci_ints(delta_p)
```

And looking at the odd ratios:

```{r}
bar_gb %>%
    halfeye(bOR, PGS) +
    coord_cartesian(xlim = c(-1, 5)) +
    geom_vline(xintercept = 1, linetype = 2)
```

Numbers for this:

```{r}
bar_gb %>%
    group_by(PGS) %>%
    ci_ints(bOR)
```

### For a surgeon most affected by PGS

```{r, warning = FALSE}
hole_bps <- precis(hole_mod, depth = 2, pars = paste0("bP[", 1:nsurgs, "]")) %>%
    as_tibble(rownames = "var")
# Least affected surgeon:
slice_min(hole_bps, mean)
# Most affected surgeon:
slice_max(hole_bps, mean)
```

Surgeon 2 is most affected.


```{r}
surg2_gb <- tidy_hole_samples %>%
    filter(surgeon == "2", PGS != 1) %>%
    mutate(PGS = as.factor(PGS))

surg2_gb %>%
    halfeye(delta_p, PGS) +
    coord_cartesian(xlim = c(-0.2, 0.7)) +
    labs(
        y = "PGS",
        x = "Added probability of a gallbladder hole"
    )
```

And numbers for this:

```{r}
surg2_gb %>%
    group_by(PGS) %>%
    ci_ints(delta_p)
```

And looking at odds ratios:

```{r}
surg2_gb %>%
    halfeye(bOR, PGS) +
    coord_cartesian(xlim = c(-4, 5)) +
    geom_vline(xintercept = 1, linetype = 2)
```

Numbers for this:

```{r}
surg2_gb %>%
    group_by(PGS) %>%
    ci_ints(bOR)
```

And comparing to the average surgeon:

```{r}
tidy_hole_samples %>%
    filter(surgeon %in% c("bar", "2")) %>%
    mutate(surgeon = if_else(surgeon == "bar", "Average", "Most affected surgeon")) %>%
    #mutate(surgeon = parse_factor(surgeon, levels = c("bar", as.character(1:nsurgs)))) %>%
    ggplot(aes(PGS, p)) +
    stat_lineribbon() +
    facet_wrap(~ surgeon) +
    scale_fill_brewer(labels = c("95%", "80%", "50%"), palette = "Purples") +
    scale_y_continuous(labels = scales::percent_format()) +
    labs(
        x = "Parkland Grading Scale",
        y = "Probability of an Inadvertent Gallbladder Hole",
        fill = "Compatibility\nIntervals"
    )
ggsave("../output/pgs_hole.svg", width = 6, height = 4)
ggsave("../output/pgs_hole.pdf", width = 6, height = 4)
```

numbers for average surgeon:

```{r}
tidy_hole_samples %>%
    filter(surgeon == "bar") %>%
    group_by(PGS) %>%
    ci_ints(p)
```

numbers for most affected surgeon:

```{r}
tidy_hole_samples %>%
    filter(surgeon == "2") %>%
    group_by(sample_num, PGS) %>%
    ci_ints(p)
```

```{r}
tidy_hole_samples %>%
    filter(surgeon == "2")
```

### Correlation of PGS1 and effect of incremental PGS

```{r}
extract_samples(hole_mod, pars = c("Rho_sid[1,2]")) %>%
    rename(rho = Rho_sid_1_2) %>%
    ci_ints(rho) %>%
    knitr::kable(
        caption = "Correlation between GB hole in PGS1 case and the effect of incrementing PGS"
    )
```

# Critical View of Safety Attainment Analysis

## Priors determination
As before, we will use weakly regularizing priors.

### Intercept

We will reuse the N(0, 1.5) prior we established for GB hole:

```{r}
p <- rnorm(1000, 0, 1.5)
hist(inv_logit(p))
```

### PGS coefficient

Remember, this is the effect of a PGS5.
We will need to simulate the intercept then add the slope,
followed by undoing the logit to see the probability:

```{r}
pint <- rnorm(1000, 0, 1.5)
ppgs <- rnorm(1000, 0, 2)
pgsdiff <- inv_logit(pint + ppgs) - inv_logit(pint)
hist(pgsdiff)
```

This prior, N(0, 2), is a wide and rather uninformative prior,
but still fair.
It allows for some surgeons to obtain CVS for a PGS1 but for PGS5
to never obtain it (due to technique preference).
However, it still clusters most of the effects around zero,
as we would, a priori, predict.

### other priors
Other priors will be the usual weakly regularizing ones,
including Dirichlet of 2,
Exponential 1,
and LKJCorr of 4.


## Formula

Below is the centered version.
The model given to Stan is the non-centered version
that is mathematically equivalent but dramatically improves sampling.

\begin{equation}
\begin{split}
CVS_i &\sim Bernoulli(p_i) \\
logit(p_i) &= \alpha_{sid[i]} + \beta_{sid[i]} * \sum_{j = 0}^{PGS_i - 1} \delta_j \\
\begin{bmatrix}
    \alpha_{sid} \\
    \beta_{sid} \\
\end{bmatrix} &\sim MVNormal(
    \begin{bmatrix}
        \alpha \\
        \beta
    \end{bmatrix},
    \mathbf{S}
    ) \\
\alpha &\sim Normal(0, 1.5) \\
\beta &\sim Normal(0, 2) \\
\delta &\sim Dirichlet(2) \\
\mathbf{S} &=
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} 
  \mathbf{R}
  \begin{pmatrix}
    \sigma_{\alpha} & 0 \\
    0 & \sigma_{\beta}
  \end{pmatrix} \\
\mathbf{R} &=
  \begin{pmatrix}
    1 & \rho \\
    \rho & 1
  \end{pmatrix} \\
\mathbf{R} &\sim LKJCorr(4) \\
\sigma_{\alpha}, \sigma_{\beta} &\sim Exponential(1)
\end{split}
\end{equation}

## Code

```{r}
set.seed(1234)
cvs_mod <- ulam(
    alist(
        cvs ~ bernoulli(p),
        logit(p) <- a_bar + ab_sid[sid, 1] + (bP_bar + ab_sid[sid, 2]) * sum(delta_j[1:pgs]),
        a_bar ~ normal(0, 1.5),
        bP_bar ~ normal(0, 2),
        vector[5]: delta_j <<- append_row(0, delta),
        simplex[4]: delta ~ dirichlet(alpha),
        transpars> matrix[sid, 2]: ab_sid <-
            compose_noncentered(sigma_sid, L_Rho_sid, z_sid),
        matrix[2, sid]: z_sid ~ normal(0, 1),
        vector[2]: sigma_sid ~ exponential(1),
        cholesky_factor_corr[2]: L_Rho_sid ~ lkj_corr_cholesky(4),
        gq> matrix[2, 2]: Rho_sid <<- Chol_to_Corr(L_Rho_sid),
        gq> vector[sid]:a <<- a_bar + ab_sid[, 1],
        gq> vector[sid]:bP <<- bP_bar + ab_sid[, 2]
    ),
    data = cdat,
    cores = 4,
    chains = 4,
    iter = 5000,
    log_lik = TRUE
)
```

## Diagnostic Evaluation of Markov Chains

### Rhat4 and effective sampling size

```{r}
precis(cvs_mod, depth = 3)
```

All Rhat4 values are 1.

Each parameter also sampled well.

### PSIS/WAIC

```{r}
PSIS(cvs_mod)
WAIC(cvs_mod)
```

The are some Pareto k values > 0.5. As long as < 0.7, not an issue.
Are they?

```{r}
PSIS(cvs_mod, pointwise = TRUE) %>%
    high_k_rows()
```

Only 1 row with a k of 0.506,
so minimal issue with outliers.

### Trace rank plot (trankplot)

```{r, eval = FALSE}
trankplot(cvs_mod)
```

Good mixing as shown with large amount of overlap.

### Trace plot

```{r, eval = FALSE}
traceplot(cvs_mod)
```

Chains are stationary with a visible central tendency,
have good mixing, and converge.

## Evaluate model results
First, obtain tidy samples, as we did with the duration and GB hole model.
Additionally, transform the result to get the absolute probability of
CVS and the OR of a obtained CVS for each PGS:

```{r}
tidy_cvs_samples <- extract_samples(
    cvs_mod,
    pars = c(
        "a_bar",
        paste0("a[", 1:nsurgs, "]"),
        "bP_bar",
        paste0("bP[", 1:nsurgs, "]"),
        # sigma_a
        "sigma_sid[1]",
        # sigma_b
        "sigma_sid[2]",
        paste0("Rho_sid[1,2]"),
        paste0("delta[", 1:4, "]")
    )
) %>%
    tidy_surgeons() %>%
    sum_deltas() %>%
    tidy_pgs() %>%
    mutate(
        p = inv_logit(a + bP * sum_delta_j),
        bOR = exp(bP * sum_delta_j)
    ) %>%
    arrange(sample_num, surgeon, PGS) %>%
    group_by(surgeon, sample_num) %>%
    # calc change in probability for each sample
    # group by surgeon as well because each sample_num
    # has the info for all 10 surgeons and bar surgeon
    mutate(
        # change from one PGS level to the other
        incr_delta_p = p - lag(p),
        # change from PGS1 to that PGS level
        delta_p = p - p[1]
    ) %>%
    ungroup()
```

Key to know, is that:

`incr_delta_p` is the probability difference from one level to the next.
It will be `NA` for a PGS1.

`delta_p` is probability difference from a certain PGS level to PGS1.

`p` is the probability of a cvs at that PGS level

`bOR` is the increased odds, compared to a PGS of 1, of obtaining CVS

## Plot results

### For all surgeons

Remember that "bar" is the average of all the surgeons:

```{r}
tidy_cvs_samples %>%
    mutate(surgeon = parse_factor(surgeon, levels = c("bar", as.character(1:nsurgs)))) %>%
    ggplot(aes(PGS, p)) +
    stat_lineribbon() +
    facet_wrap(~ surgeon) +
    scale_fill_brewer(palette = "Purples") +
    labs(y = "Probability of CVS Attainment")
```

### On average across all surgeons

```{r}
bar_cvs <- tidy_cvs_samples %>%
    filter(surgeon == "bar", PGS != 1) %>%
    mutate(PGS = as.factor(PGS))
bar_cvs %>%
    halfeye(delta_p, PGS) +
    coord_cartesian(xlim = c(-0.6, 0.2)) +
    labs(
        y = "PGS",
        x = "Change in Probability from\nPGS1 of Obtaining CVS"
    )
```

And numbers for this:

```{r}
bar_cvs %>%
    group_by(PGS) %>%
    ci_ints(delta_p)
```

And looking at the odd ratios:

```{r}
bar_cvs %>%
    halfeye(bOR, PGS) +
    coord_cartesian(xlim = c(-0.1, 2)) +
    geom_vline(xintercept = 1, linetype = 2)
```

Numbers for this:

```{r}
bar_cvs %>%
    group_by(PGS) %>%
    ci_ints(bOR)
```

### For a surgeon particularly affected by PGS

```{r}
surg4_cvs <- tidy_cvs_samples %>%
    filter(surgeon == "4", PGS != 1) %>%
    mutate(PGS = as.factor(PGS))

surg4_cvs %>%
    halfeye(delta_p, PGS) +
    coord_cartesian(xlim = c(-0.6, 0.3)) +
    labs(
        y = "PGS",
        x = "Probability change of CVS Attainment\nFrom PGS1"
    )
```

And numbers for this:

```{r}
surg4_cvs %>%
    group_by(PGS) %>%
    ci_ints(delta_p)
```

And looking at odds ratios:

```{r}
surg4_cvs %>%
    halfeye(bOR, PGS) +
    coord_cartesian(xlim = c(-0.1, 3)) +
    geom_vline(xintercept = 1, linetype = 2)
```

Numbers for this:

```{r}
surg4_cvs %>%
    group_by(PGS) %>%
    ci_ints(bOR)
```

### Average and surgeon most affected by PGS

```{r}
tidy_cvs_samples %>%
    filter(surgeon %in% c("bar", "4"), PGS != 1) %>%
    mutate(
        surgeon = if_else(surgeon == "bar", "Average", "Most affected surgeon"),
        PGS = as.factor(PGS)
    ) %>%
    halfeye(bOR, PGS) +
    facet_wrap(~ surgeon) +
    coord_cartesian(xlim = c(-0.1, 2)) +
    geom_vline(xintercept = 1, linetype = 2) +
    labs(
        x = "Odds Ratio of Attaining the Critical View of Safety",
        y = "Parkland Grading Scale"
    )
ggsave("../output/pgs_cvs.svg", width = 6, height = 4)
ggsave("../output/pgs_cvs.pdf", width = 6, height = 4)
```

### Correlation of PGS1 and effect of incremental PGS on CVS

```{r}
extract_samples(cvs_mod, pars = c("Rho_sid[1,2]")) %>%
    rename(rho = Rho_sid_1_2) %>%
    ci_ints(rho) %>%
    knitr::kable(
        caption = "Correlation between attaining CVS in PGS1 case and the effect of incrementing PGS"
    )
```


# Summarise inputs/outcomes

## Numbers

Number of surgeons:

```{r}
nsurgs
```

Number of videos with surgeons >= 5 cases:

```{r}
nrow(dat)
```

## PGS

```{r}
dat %>%
    ggplot(aes(pgs)) +
    geom_bar(fill = histo_color) +
    labs(
        x = "Parkland Grading Scale",
        y = "Number of videos"
    )
ggsave("../output/pgs_distribution.svg", width = 6, height = 6)
ggsave("../output/pgs_distribution.pdf", width = 6, height = 6)
```

```{r}
dat %>%
    count(pgs) %>%
    mutate(prop = n / sum(n)) %>%
    mutate(cum.sum = cumsum(prop))
```

## Laparoscopic duration

```{r}
dat %>%
    ggplot(aes(y = "", x = laparascopic_duration)) +
    stat_halfeye(
            aes(
                fill = stat(
                    cut_cdf_qi(cdf, .width = c(0.5, 0.8, 0.99), labels = scales::percent_format())
                )
            ),
            .width = c(0.5, 0.8, 0.99)
        ) +
        scale_fill_brewer(direction = -1, palette = "Purples", na.translate = FALSE) +
    labs(fill = "% of cases", y = "", x = "Laparoscopic duration (min)") +
    coord_cartesian(xlim = c(-5, 150))
```

```{r}
summary(dat$laparascopic_duration)
```

## CVS number of cases with attainment

```{r}
hist(cdat$cvs)
mean(cdat$cvs)
```

## GB holes

```{r}
hist(gdat$hole)
mean(gdat$hole)
```

# Environment

```{r}
devtools::session_info()
```
